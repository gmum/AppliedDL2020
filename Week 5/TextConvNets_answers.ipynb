{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TextConvNets_answers.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"QAIWCXovhqEF","colab_type":"code","colab":{}},"source":["try:\n","    # %tensorflow_version only exists in Colab.\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n4NRQj0gC3_L","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import math\n","import time\n","import itertools\n","\n","import tensorflow as tf\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from tensorflow import keras\n","from sklearn.preprocessing import OneHotEncoder\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKfCnsgRumC8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KbGkOYhTumck","colab_type":"text"},"source":["# Text processing"]},{"cell_type":"markdown","metadata":{"id":"6bxFsZWcu2oD","colab_type":"text"},"source":["Deep learning models are used to process various NLP tasks, such as:\n","\n","\n","*   Sentiment classification\n","*   POS tagging\n","*   Translation\n","*   Sequence labeling\n","*   Language inference\n","*   Question answering\n","*   Image captioning\n","\n"]},{"cell_type":"code","metadata":{"id":"XCN8gj7cwU0V","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZevEqu9KwU7s","colab_type":"text"},"source":["## Text preprocessing before training"]},{"cell_type":"markdown","metadata":{"id":"eb12H8k0xqmA","colab_type":"text"},"source":["Before processing text with the model, we have to make the preprocessing, which usually takes steps like:\n","\n","\n","1.   Convert text to lowercase\n","2.   Numbers removing / replacing with token *NUM*\n","3.   Remove punctuation\n","4.   Tokenization\n","5.   Remove stop words\n","6.   Remove common / rare words\n","7.   Stemming / Lemmatization\n","8.   Split word into pieces (playing -> play + ing)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rh3rA69C5xFY","colab_type":"text"},"source":["### Embeddings"]},{"cell_type":"markdown","metadata":{"id":"1Pl-jmCA7rrZ","colab_type":"text"},"source":["Usually after the preprocessing, we map each word from the training corpora into the integer number. Then this number is mapped into the vector representation, named word embedding.\n","\n","Examplary embeddings that we could use in our models:\n","\n","1.   One-hot-encoding / tf-idf\n","2.   LSA / LDA\n","3.   Word2vec / GloVe\n","4.   Trained embedding layer\n","\n","![](https://cdn-images-1.medium.com/max/800/1*_kDJnuzDA5SiQ9N0tmJRbw.png)\n","\n","In very simplistic terms, Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text. The best words embeddings could model word analogies and word similarities from the corpora.\n","\n","![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2017/06/06062705/Word-Vectors.png)\n","\n","In our tasks, we could use pretrained embeddings (usually in unsupervised manner) or train it from scratch and have the task specific embedding."]},{"cell_type":"markdown","metadata":{"id":"gFf8UHupDtLR","colab_type":"text"},"source":["You could visualise embeddings in [Embedding Projector](https://projector.tensorflow.org/)."]},{"cell_type":"code","metadata":{"id":"RGwKPv8eDTer","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R9VlJhfZ50Of","colab_type":"text"},"source":["### Padding"]},{"cell_type":"markdown","metadata":{"id":"3X6xoZxO41sy","colab_type":"text"},"source":["All inputs of neural networks has to be of the same shape. In order to fulfill this, we pad all our sentences with *blank* token.\n","We could pad sequences before the training (then we pad all sentences to the length of the longest sentence in the training dataset) or online, during the training (then we pad all batch sentences to the length of the longest sentence in the training bacth)."]},{"cell_type":"code","metadata":{"id":"KAtZeAUKwa-P","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ySo1V3TLwnnj","colab_type":"text"},"source":["## Deep learning models for NLP"]},{"cell_type":"markdown","metadata":{"id":"6xm0i9o7wyAA","colab_type":"text"},"source":["### Recurrent neural networks"]},{"cell_type":"markdown","metadata":{"id":"RWGuSy6bJgAN","colab_type":"text"},"source":["![](http://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)"]},{"cell_type":"code","metadata":{"id":"FzOSUN68wbEF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NdSZ-lsCw6Em","colab_type":"text"},"source":["### Convolutional neural networks"]},{"cell_type":"markdown","metadata":{"id":"ORpQmIA0J9Py","colab_type":"text"},"source":["![](https://cdn-images-1.medium.com/max/1800/1*aBN2Ir7y2E-t2AbekOtEIw.png)"]},{"cell_type":"markdown","metadata":{"id":"_kapCypJLjZP","colab_type":"text"},"source":["![](http://www.joshuakim.io/wp-content/uploads/2017/12/filtering2.jpg)"]},{"cell_type":"markdown","metadata":{"id":"qP9HDqxALuxG","colab_type":"text"},"source":["![](https://www.researchgate.net/profile/Qingcai_Chen/publication/273471942/figure/fig1/AS:281712618688515@1444176930410/The-over-all-architecture-of-the-convolutional-sentence-model-A-box-with-dashed-lines.png)"]},{"cell_type":"code","metadata":{"id":"7UbpRsJEw9NM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VV5mLZepw91P","colab_type":"text"},"source":["### Self attention based models"]},{"cell_type":"markdown","metadata":{"id":"sRl7ad4QJ1KI","colab_type":"text"},"source":["![](http://nlp.seas.harvard.edu/images/the-annotated-transformer_38_0.png)\n","![](http://nlp.seas.harvard.edu/images/the-annotated-transformer_33_0.png)"]},{"cell_type":"code","metadata":{"id":"T4qvQnluwbBW","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CswqcRb_wbtv","colab_type":"text"},"source":["## IMDB Movie reviews dataset for sentiment classification"]},{"cell_type":"markdown","metadata":{"id":"SZea-sYtMlsM","colab_type":"text"},"source":["**Overview**\n","\n","This dataset contains movie reviews along with their associated binary sentiment polarity labels. It is intended to serve as a benchmark for sentiment classification.\n","\n","**Dataset**\n","\n","The core dataset contains 50,000 reviews split evenly into 25k train and 25k test sets. The overall distribution of labels is balanced (25k pos and 25k neg).\n","\n","In the entire collection, no more than 30 reviews are allowed for any given movie because reviews for the same movie tend to have correlated ratings. Further, the train and test sets contain a disjoint set of movies, so no significant performance is obtained by memorizing movie-unique terms and their associated with observed labels. In the labeled train/test sets, a negative review has a score <= 4 out of 10, and a positive review has a score >= 7 out of 10. Thus reviews with more neutral ratings are not included in the train/test sets."]},{"cell_type":"code","metadata":{"id":"KsX29pt5xMMx","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"24cyT711xMlb","colab_type":"text"},"source":["# Sentiment classification with 1D CNNs"]},{"cell_type":"markdown","metadata":{"id":"4OG1oMtGNBA_","colab_type":"text"},"source":["## Load the data and make padding\n","\n","Keras provides us with the [imdb dataset](https://keras.io/datasets/). The dataset is preprocessed and loaded data consists of the different shaped lists with integers (words indices). During the loading we could preprocess it further to get sequences with specified lenght and truncate all words that occurs rare.\n","\n","We could also use keras to preprocess the data more and [pad](https://keras.io/preprocessing/sequence/) all our sequences.\n"]},{"cell_type":"code","metadata":{"id":"Isjo4-U_DThH","colab_type":"code","colab":{}},"source":["from keras.datasets import imdb\n","\n","(x_train, y_train), (x_test, y_test) = imdb.load_data()\n","\n","x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=None, padding='post')\n","\n","MAX_SEQUENCE_LEN = x_train.shape[1]\n","WORDS_IN_CORPORA = np.max(x_train) + 1\n","\n","x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_SEQUENCE_LEN, padding='post', truncating='post')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFX8IZVEDTjq","colab_type":"code","colab":{}},"source":["x_train.shape, y_train.shape, x_test.shape, y_test.shape, WORDS_IN_CORPORA"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n5_TyZAhKi2B","colab_type":"code","colab":{}},"source":["words2index = imdb.get_word_index()\n","index2word = {v: k for k,v in words2index.items()}\n","imdb_words = [v for k,v in sorted(index2word.items())]\n","len(imdb_words)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rb4g27O6MGhJ","colab_type":"code","colab":{}},"source":["imdb_words[0:5]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0eof5zPS64_C","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0TpjA-zfO3RL","colab_type":"text"},"source":["## Define the network"]},{"cell_type":"markdown","metadata":{"id":"DHYdkRoRX1Yo","colab_type":"text"},"source":["Define the CNN for text classification that consists of:\n","\n","1.   Input layer [keras.layers.Input](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Input) that will process the input sentences, remember about the proper input shape.\n","2.   Embedding layer [keras.layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) for training the word embeddings. You should pass the propper input dim and length to the layer and specify the embedding dim.\n","3.   Some 1D convolutional layers [keras.layers.Conv1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) with specified number of filters, kernel size, stride, activation function. After the convolution you can use the batch norm layer [keras.layers.BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization).\n","4.   Together with 1D convolutions, you can also use the 1D max pooling layers [keras.layers.MaxPooling1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool1D), to decrease the input length.\n","5.   After some convolutions and poolings, you should use the 1D global max pooling [keras.layers.GlobalMaxPool1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D) to be sure that the each sequence has specified shape (1 x num_channels)\n","6.   Layer that will flatten the result of convolutions [keras.layers.Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten).\n","7.   Some Dense layers [keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), with specified number of units and activation function. After dense layers you could also use the batch norm layer.\n","8.   A final Dense layer with 2 neurons (one per class 0 & 1)."]},{"cell_type":"markdown","metadata":{"id":"i3PqDgZCVxB2","colab_type":"text"},"source":["I used the following architecture and obtained ~89% accuracy. But you can experiment with your own architectures.\n","\n","1.  Embedding layer with embedding dim equal to 100.\n","2.  3 times repeated the following layers sequence:\n","\n","    1.    1D conv with 128 filters, kernel size equal to 5, stride equal to 1, relu activation and same padding.\n","    2.    Batch norm\n","    3.   1D max pooling with pool_size equal to 5\n","\n","3.   After flatten, before the final layer, I use dense layer with 128 units and relu activation, followed by batch norm.\n"]},{"cell_type":"markdown","metadata":{"id":"vm_bSLoHc4yo","colab_type":"text"},"source":["**Define the input layer**"]},{"cell_type":"code","metadata":{"id":"Ez9DtUmdc5NZ","colab_type":"code","colab":{}},"source":["sequence_input = keras.layers.Input(shape=(MAX_SEQUENCE_LEN,), dtype='int32')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Xhvarpzc9Cn","colab_type":"text"},"source":["**Define the embedding layer**"]},{"cell_type":"code","metadata":{"id":"ayHUaKQ-c9Ki","colab_type":"code","colab":{}},"source":["embedded_sequences = keras.layers.Embedding(input_dim=WORDS_IN_CORPORA,\n","                                            output_dim=100,\n","                                            input_length=MAX_SEQUENCE_LEN)(sequence_input)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b5J-b2UwdGBK","colab_type":"text"},"source":["**Define all convolutional and pooling layers**"]},{"cell_type":"code","metadata":{"id":"oOEaatkHDTlx","colab_type":"code","colab":{}},"source":["x = keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(embedded_sequences)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.MaxPooling1D(pool_size=5)(x)\n","x = keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(x)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.MaxPooling1D(pool_size=5)(x)\n","x = keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(x)\n","x = keras.layers.BatchNormalization()(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2zVmIjvjdWVg","colab_type":"text"},"source":["**Define the global max pooling layer, together with flatten layer**"]},{"cell_type":"code","metadata":{"id":"6Dth8_QqdZln","colab_type":"code","colab":{}},"source":["x = keras.layers.GlobalMaxPool1D()(x)\n","x = keras.layers.Flatten()(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RilrhYTidL-l","colab_type":"text"},"source":["**Define all dense layers**"]},{"cell_type":"code","metadata":{"id":"naCrI7osdMiG","colab_type":"code","colab":{}},"source":["x = keras.layers.Dense(128, activation='relu')(x)\n","x = keras.layers.BatchNormalization()(x)\n","sequence_output = keras.layers.Dense(2, activation='softmax')(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"imi7GKHFdRSY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MGyEXAn9efXJ","colab_type":"text"},"source":["**Define keras model**\n","\n","You should pass the proper input and output tensors to the initializer."]},{"cell_type":"code","metadata":{"id":"4TVzf3vUteqN","colab_type":"code","colab":{}},"source":["model = keras.models.Model(inputs=[sequence_input], outputs=[sequence_output])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9wZGJRRJefPZ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0EO3BHCUdRYz","colab_type":"text"},"source":["**Check the model summary**"]},{"cell_type":"code","metadata":{"id":"jf0OleEtxx4_","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nt-TITnaerpA","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lodwdIYnhkcS","colab_type":"text"},"source":["## Train the network"]},{"cell_type":"markdown","metadata":{"id":"nX65arlMer3z","colab_type":"text"},"source":["**Before training, you should compile the model with a propper loss function and optimizer**\n","\n","You could experiment with different optimizers, with different learning rates and [lr schedulers](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler)."]},{"cell_type":"code","metadata":{"id":"_1VblmIa7EMX","colab_type":"code","colab":{}},"source":["model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam', \n","              metrics=[\"accuracy\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hWEveXixe45r","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WdopgFjxe5A1","colab_type":"text"},"source":["**Train the model**"]},{"cell_type":"code","metadata":{"id":"-3TqQWWw7Ey1","colab_type":"code","colab":{}},"source":["model.fit(x_train, y_train, epochs=5, batch_size=32,\n","          validation_data=(x_test, y_test),\n","          callbacks=[tf.keras.callbacks.LearningRateScheduler(schedule = lambda x: 0.001 if x == 0 else 0.0001)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KgDL6dZ0IItj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rUY0RUyKII7S","colab_type":"text"},"source":["**Save word embeddings**\n","\n","You could use trained embedding layer to save word embeddings.\n","Then you could visualise the embedding space in e.g. [Embedding Projector](https://projector.tensorflow.org/)."]},{"cell_type":"code","metadata":{"id":"d_FPePVJ7E1I","colab_type":"code","colab":{}},"source":["words_sample = np.arange(2, 1002, dtype=np.int32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ckIjWVYiPx94","colab_type":"code","colab":{}},"source":["model.submodules"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pr2F1Hw0PCqP","colab_type":"code","colab":{}},"source":["words_embeddings = model.submodules[1](words_sample)\n","words_embeddings"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DEm_r7igP4D7","colab_type":"code","colab":{}},"source":["np.savetxt(\"words_embeddings.tsv\", words_embeddings.numpy(), delimiter=\"\\t\")\n","np.savetxt(\"words.tsv\", np.array(imdb_words[:1000]).reshape((-1,1)), fmt='%s')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lYipQoh691X8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xqhcoim1Smxa","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1ct_eNJPgLNF","colab_type":"text"},"source":["# 1D CNNs with various kernel sizes\n","\n","[Kim - Convolutional Neural Networks for Sentence Classification](https://www.aclweb.org/anthology/D14-1181)\n","\n","Instead of working with convolutions with a single kernel size, we could also use convolutions with different kernel sizes to work with our text data. "]},{"cell_type":"markdown","metadata":{"id":"g79D-yiG911h","colab_type":"text"},"source":["![](https://richliao.github.io/images/YoonKim_ConvtextClassifier.png)"]},{"cell_type":"markdown","metadata":{"id":"d_dfH6gQaici","colab_type":"text"},"source":["Define the CNN for text classification that consists of:\n","\n","1.   Input layer [keras.layers.Input](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Input) that will process the input sentences, remember about the proper input shape.\n","2.   Embedding layer [keras.layers.Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding) for training the word embeddings. You should pass the propper input dim and length to the layer and specify the embedding dim.\n","3.  Some 1D convolutional layers [keras.layers.Conv1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv1D) with specified number of filters, stride, activation function, but with **different kernel sizes**. After the convolution you can use the batch norm layer [keras.layers.BatchNormalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization).\n","4.   Together with 1D convolutions, you can also use the 1D max pooling layers [keras.layers.MaxPooling1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool1D), to decrease the input length.\n","5.   After these convolutions you should concatenate their outputs. You can do it with concatenate layer [keras.layers.Concatenate](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate).\n","6.   On concatenated outputs You could again use 1D convolutional layers, with different or one kernel size, together with batch norm and pooling.\n","5.   After some convolutions and poolings, you should use the 1D global max pooling [keras.layers.GlobalMaxPool1D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalMaxPool1D) to be sure that the each sequence has specified shape (1 x num_channels)\n","6.   Layer that will flatten the result of convolutions [keras.layers.Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten).\n","7.   Some Dense layers [keras.layers.Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), with specified number of units and activation function. After dense layers you could also use the batch norm layer.\n","8.   A final Dense layer with 2 neurons (one per class 0 & 1)."]},{"cell_type":"markdown","metadata":{"id":"OewlXB5NcYPb","colab_type":"text"},"source":["I used the following architecture and obtained ~90% accuracy. But you can experiment with your own architectures.\n","\n","1.  Embedding layer with embedding dim equal to 100.\n","2.  The following layers sequence:\n","    1.   1D convolutions with filter sizes 3,4,5\n","    2.    Batch norm for the output of every convolution\n","    3.   1D max pooling with pool_size equal to 5 for the output of every convolution\n","    \n","3.  2 times repeated the following layers sequence:\n","\n","    1.    1D conv with 128 filters, kernel size equal to 5, stride equal to 1, relu activation and same padding.\n","    2.    Batch norm\n","    3.   1D max pooling with pool_size equal to 5\n","\n","4.   After flatten, before the final layer, I use dense layer with 128 units and relu activation, followed by batch norm.\n"]},{"cell_type":"code","metadata":{"id":"aVMBNiqGdGJD","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJRVKS1udGRI","colab_type":"text"},"source":["**Define the input layer**"]},{"cell_type":"code","metadata":{"id":"W6oKUXZncXnA","colab_type":"code","colab":{}},"source":["sequence_input = keras.layers.Input(shape=(MAX_SEQUENCE_LEN,), dtype='int32')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KkFYnKM-dJbZ","colab_type":"text"},"source":["**Define the embedding layer**"]},{"cell_type":"code","metadata":{"id":"3_ZuywjldJkZ","colab_type":"code","colab":{}},"source":["embedded_sequences = keras.layers.Embedding(input_dim=WORDS_IN_CORPORA,\n","                                            output_dim=100,\n","                                            input_length=MAX_SEQUENCE_LEN)(sequence_input)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q-zFQkK2dJxJ","colab_type":"text"},"source":["**Define convolutional layers with different kernel sizes**"]},{"cell_type":"code","metadata":{"id":"hkqDNwuldVOE","colab_type":"code","colab":{}},"source":["filter_sizes = [3,4,5]\n","convs = []\n","\n","for size in filter_sizes:\n","    x = keras.layers.Conv1D(filters=128, kernel_size=size, activation='relu', padding='same')(embedded_sequences)\n","    x = keras.layers.BatchNormalization()(x)\n","    x = keras.layers.MaxPooling1D(pool_size=5)(x)\n","    convs.append(x)\n","\n","x = keras.layers.Concatenate()(convs)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N9DYU2FodVXk","colab_type":"text"},"source":["\n","**Define all others convolutional and pooling layers**"]},{"cell_type":"code","metadata":{"id":"lCCLoyjadJ7Q","colab_type":"code","colab":{}},"source":["x = keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(x)\n","x = keras.layers.BatchNormalization()(x)\n","x = keras.layers.MaxPooling1D(pool_size=5)(x)\n","x = keras.layers.Conv1D(filters=128, kernel_size=5, activation='relu', padding='same')(x)\n","x = keras.layers.BatchNormalization()(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rdegnB3hdkJK","colab_type":"text"},"source":["**Define the global max pooling layer, together with flatten layer**"]},{"cell_type":"code","metadata":{"id":"hg71PlMwdk7V","colab_type":"code","colab":{}},"source":["x = keras.layers.GlobalMaxPool1D()(x)\n","x = keras.layers.Flatten()(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mx66RaAGdsGU","colab_type":"text"},"source":["**Define all dense layers**"]},{"cell_type":"code","metadata":{"id":"dMibQd_JdsVS","colab_type":"code","colab":{}},"source":["x = keras.layers.Dense(128, activation='relu')(x)\n","x = keras.layers.BatchNormalization()(x)\n","sequence_output = keras.layers.Dense(2, activation='softmax')(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kiyeymcpdxhQ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JUWQlOFCreAq","colab_type":"text"},"source":["**Define keras model**\n","\n","You should pass the proper input and output tensors to the initializer."]},{"cell_type":"code","metadata":{"id":"hhmrzg6v7OCV","colab_type":"code","colab":{}},"source":["model = keras.models.Model(inputs=[sequence_input], outputs=[sequence_output])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8tZ1ZZNMiXME","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v4Q0-3Fwrlgt","colab_type":"text"},"source":["**Check the model summary**"]},{"cell_type":"code","metadata":{"id":"x1MkeyQurnCK","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3rpgrgP1iXc5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zbGgijeGiX18","colab_type":"text"},"source":["## Train the network"]},{"cell_type":"markdown","metadata":{"id":"4Bo1t7Toia84","colab_type":"text"},"source":["You could use classic tf.keras training approach (model compile and fit).\n","\n","You could experiment with different optimizers, with different learning rates and [lr schedulers](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler)."]},{"cell_type":"code","metadata":{"id":"-ax6zvjC7N_a","colab_type":"code","colab":{}},"source":["model.compile(loss='sparse_categorical_crossentropy',\n","              optimizer='adam', \n","              metrics=[\"accuracy\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5M7HXy0l71Tj","colab_type":"code","colab":{}},"source":["model.fit(x_train, y_train, epochs=5, batch_size=32,\n","          validation_data=(x_test, y_test),\n","          callbacks=[tf.keras.callbacks.LearningRateScheduler(schedule = lambda x: 10 ** (-x-3))])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CmVylUvYjBOB","colab_type":"text"},"source":["Or use more [TensorFlow like training](https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation#part_ii_writing_your_own_training_evaluation_loops_from_scratch)."]},{"cell_type":"code","metadata":{"id":"oaluUSsB71QJ","colab_type":"code","colab":{}},"source":["model = keras.models.Model(inputs=[sequence_input], outputs=[sequence_output])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0kQPILEZseZI","colab_type":"code","colab":{}},"source":["# Instantiate an optimizer.\n","optimizer = keras.optimizers.SGD(lr=1e-3)\n","\n","# Instantiate a loss function.\n","loss_fn = keras.losses.SparseCategoricalCrossentropy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"me5AbZBjsi8V","colab_type":"code","colab":{}},"source":["# Prepare the training dataset.\n","batch_size = 32\n","train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","train_dataset = train_dataset.shuffle(buffer_size=25000).batch(batch_size)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J14tbwQCQFT0","colab_type":"code","colab":{}},"source":["# Iterate over epochs.\n","for epoch in range(5):\n","    print('Start of epoch %d' % (epoch,))\n","  \n","    # Iterate over the batches of the dataset.\n","    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n","\n","        # Open a GradientTape to record the operations run\n","        # during the forward pass, which enables autodifferentiation.\n","        with tf.GradientTape() as tape:\n","\n","            # Run the forward pass of the layer.\n","            # The operations that the layer applies\n","            # to its inputs are going to be recorded\n","            # on the GradientTape.\n","            logits = model(x_batch_train)  # Logits for this minibatch\n","\n","            # Compute the loss value for this minibatch.\n","            loss_value = loss_fn(y_batch_train, logits)\n","\n","            # Use the gradient tape to automatically retrieve\n","            # the gradients of the trainable weights with respect to the loss.\n","            grads = tape.gradient(loss_value, model.trainable_variables)\n","\n","            # Run one step of gradient descent by updating\n","            # the value of the weights to minimize the loss.\n","            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","\n","        # Log every 200 batches.\n","        if step % 200 == 0:\n","            print('Training loss (for one batch) at step %s: %s' % (step, float(loss_value)))\n","            print('Seen so far: %s samples' % ((step + 1) * 64))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-PmbcATdRfNk","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gXWlENTEG5f9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8gWyyyNMG5pw","colab_type":"text"},"source":["# Images sources\n","\n","Images used in this notebook comes from the following web pages and papers:\n","\n","1.   https://medium.com/datadriveninvestor/neural-networks-or-deep-learning-in-natural-language-processing-f7b534a14728\n","2.   http://www.joshuakim.io/understanding-how-convolutional-neural-network-cnn-perform-text-classification-with-word-embeddings/\n","3.   https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf\n","4.   https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n","5.   https://www.researchgate.net/figure/The-over-all-architecture-of-the-convolutional-sentence-model-A-box-with-dashed-lines_fig1_273471942\n","6.   http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n","7.   http://nlp.seas.harvard.edu/2018/04/03/attention.html\n","8.   [Kim publication](https://www.aclweb.org/anthology/D14-1181)"]},{"cell_type":"code","metadata":{"id":"M7o1oFiCHrtg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}