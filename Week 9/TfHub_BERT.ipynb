{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TfHub_BERT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPspd6uWD0AEa4fkhqhf38W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JI6-KjMQ35qb","colab_type":"text"},"source":["In this notebook we will use pretrained BERT model from TF Hub, together with Keras Sequential API, to solve the IMDB reviews classification task."]},{"cell_type":"markdown","metadata":{"id":"oQYDLjeE4e7t","colab_type":"text"},"source":["![](https://drive.google.com/uc?export=view&id=1LfLDPCHlovwwChNGPq8ArExk1uUshuVM)\n","\n","![](https://drive.google.com/uc?export=view&id=1MdNRG2Yt1OqxiPp3UsHdCW9-2lMkvzG_)"]},{"cell_type":"code","metadata":{"id":"HnmkkSbB34n5","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JYhOUId3ZC0S","colab_type":"code","colab":{}},"source":["!pip install bert-for-tf2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QarjD0nF26OQ","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","import tensorflow as tf\n","import tensorflow_hub as hub\n","import tensorflow_datasets as tfds\n","\n","from bert import bert_tokenization"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QNeL_QB02J2s","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qh2oKb2y2KG7","colab_type":"text"},"source":["# Download imdb dataset\n","\n","BERT is using its own tokenizer, because of that we have to prepare the data tensors from raw text."]},{"cell_type":"code","metadata":{"id":"g7dmeRJF3Vtv","colab_type":"code","colab":{}},"source":["train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], \n","                                  batch_size=-1, as_supervised=True)\n","\n","train_examples, train_labels = tfds.as_numpy(train_data)\n","test_examples, test_labels = tfds.as_numpy(test_data)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CgheSbIN4IAO","colab_type":"code","colab":{}},"source":["print(\"Training entries: {}, test entries: {}\".format(len(train_examples), len(test_examples)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rL_p0B3K4jQS","colab_type":"code","colab":{}},"source":["train_examples[:10]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DcacFWljJCB7","colab_type":"code","colab":{}},"source":["train_labels[:10]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YM0Fa-xpJCGn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B58XtJqK3ApU","colab_type":"text"},"source":["# Define BERT layer from TF Hub"]},{"cell_type":"markdown","metadata":{"id":"XYF6jvbUZIw6","colab_type":"text"},"source":["[BERT TF Hub documentation](https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2)\n","\n","BERT takes 3 tensors as input:\n","\n","*   input_word_ids - Tensor with word ids for all words in sequence. For all sequences in the batch.\n","*   input_mask - Tensor with information about text padding. Mask equals 0 for padding token and 1 otherwise.\n","*   segment_ids - Tensor with information abouth the segment is - whether it equals 0 or 1. Segments ids are used during BERT pretraining tasks. In our text classification they will be always equal 0.\n","\n","\n","BERT returns 2 tensors as output:\n","\n","*   pooled_output - Tensor of shape [batch_size, 768] with representations for the entire input sequences\n","*   sequence_output - Tensor of shape [batch_size, max_seq_length, 768] with representations for each input token (in context)."]},{"cell_type":"code","metadata":{"id":"EBQLfYoM4jVv","colab_type":"code","colab":{}},"source":["bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/2\",\n","                            trainable=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"__CjVa_w3Ejg","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T6xm38sW3EqV","colab_type":"text"},"source":["# Prepare data tensors\n","\n","BERT uses its own tokenization method, so that we cannot use predefined tf.keras imdb dataset and we have to use raw data. However TF Hub contains functions that could help us to prepare input tensors with proper words id that could be consumed by BERT."]},{"cell_type":"markdown","metadata":{"id":"PW1eMPCp5OHb","colab_type":"text"},"source":["In the following code snippet we use BERT tokenizer to split the raw text into tokens. And then convert these tokens into id vectors (that are handled by embedding layers)."]},{"cell_type":"code","metadata":{"id":"2sKUWKpXJCFE","colab_type":"code","colab":{}},"source":["vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n","do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZkKdDwwJs_Q","colab_type":"code","colab":{}},"source":["tokenizer = bert_tokenization.FullTokenizer(vocab_file, do_lower_case)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-VI736vaeJ6","colab_type":"code","colab":{}},"source":["def tokenize_text(txt):\n","    tokenized_txt = tokenizer.tokenize(txt)\n","    tokenized_txt = [\"[CLS]\"] + tokenized_txt + [\"[SEP]\"]\n","    tokenized_txt = tokenizer.convert_tokens_to_ids(tokenized_txt)\n","    return tokenized_txt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxbPWJsfaeMz","colab_type":"code","colab":{}},"source":["train_examples = list(map(tokenize_text, train_examples))\n","test_examples = list(map(tokenize_text, test_examples))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oxaAzL6w3G6N","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A603kDxc7EMa","colab_type":"text"},"source":["In the following code snippet we pad training and testing sequences (with max sequence lenght set to 100). And then we are preparing arrays with token masks and segment ids.\n","\n","*   Token mask is array that contains information about text padding. Mask equals 0 for padding token and 1 otherwise.\n","*   Segments ids are used during BERT pretraining tasks. In our text classification they will be always equal 0.\n","\n"]},{"cell_type":"code","metadata":{"id":"DOcfUtHAfTSi","colab_type":"code","colab":{}},"source":["x_train = tf.keras.preprocessing.sequence.pad_sequences(train_examples, maxlen=100, padding='post')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z3ieG5_zfTbb","colab_type":"code","colab":{}},"source":["max_seq_length = x_train.shape[1]\n","x_test = tf.keras.preprocessing.sequence.pad_sequences(test_examples, maxlen=max_seq_length, padding='post', truncating='post')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7v-rrbVCiSgD","colab_type":"code","colab":{}},"source":["train_mask = (x_train != 0).astype(int)\n","test_mask = (x_test != 0).astype(int)\n","\n","train_segments = np.zeros(x_train.shape)\n","test_segments = np.zeros(x_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lBwpj2n8yrwS","colab_type":"code","colab":{}},"source":["print(x_train.shape, train_mask.shape, train_segments.shape)\n","print(x_test.shape, test_mask.shape, test_segments.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJREoI974jTH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fBoePqR52qUu","colab_type":"text"},"source":["# Define input and output layers\n","\n","We have to define 3 input layers for all inputs that are passed for BERT: input_word_ids, input_mask, segment_ids.\n","\n","BERT returns 2 tensors as output:  pooled_output, sequence_output. We have to take the pooled_output, that represents the vector embedding of the sequence and pass it to the classification layer."]},{"cell_type":"code","metadata":{"id":"VaE0a7un4jxo","colab_type":"code","colab":{}},"source":["input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n","                                       name=\"input_word_ids\")\n","input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n","                                   name=\"input_mask\")\n","segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n","                                    name=\"segment_ids\")\n","\n","bert_inputs = [input_word_ids, input_mask, segment_ids]\n","pooled_output, sequence_output = bert_layer(bert_inputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ddKC1YyCmFcb","colab_type":"code","colab":{}},"source":["final_output = ###"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zAp3GwOX5CxB","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80ePoUws2t-z","colab_type":"text"},"source":["# Define and compile model"]},{"cell_type":"code","metadata":{"id":"JNZiaCMch3cx","colab_type":"code","colab":{}},"source":["model = ###"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRpE0M9Rh3f5","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SyJVEKY1iAds","colab_type":"code","colab":{}},"source":["model.compile(###)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3kj6pQ-liAlC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BGzeO94W2xnK","colab_type":"text"},"source":["# Train model"]},{"cell_type":"markdown","metadata":{"id":"xldHeW0fcRnE","colab_type":"text"},"source":["Remember that BERT takes 3 tensors as the input."]},{"cell_type":"code","metadata":{"id":"YWyhH8EbiAi9","colab_type":"code","colab":{}},"source":["model.fit(###)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G-BcZlt2iAhM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DAQgsvEqh3q1","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}