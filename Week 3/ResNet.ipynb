{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ResNet.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Ms4uHLhgG4v_","colab_type":"code","colab":{}},"source":["try:\n","    # %tensorflow_version only exists in Colab.\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oyvFJGugrdae","colab_type":"code","colab":{}},"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7zpOR7zHU_RT","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import math\n","import time\n","import datetime\n","\n","import tensorflow as tf\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","\n","from tensorflow import keras\n","from sklearn.preprocessing import OneHotEncoder\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JK-FpVSC9zci","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UofGyaQ_9zq-","colab_type":"text"},"source":["# ImageNet Challenge"]},{"cell_type":"markdown","metadata":{"id":"7VNd19-Q-9fe","colab_type":"text"},"source":["![](https://drive.google.com/uc?export=view&id=1LK_eQgSZXykX20g-T4myDVz_Lo0HOde5)"]},{"cell_type":"code","metadata":{"id":"-uBjLg4woxG0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUEzdu4E_-_u","colab_type":"text"},"source":["## AlexNet\n","\n","![](https://www.researchgate.net/profile/Jaime_Gallego2/publication/318168077/figure/fig1/AS:578190894927872@1514862859810/AlexNet-CNN-architecture-layers.png)"]},{"cell_type":"code","metadata":{"id":"V9SbSdndAW5B","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"95HTZPbUACND","colab_type":"text"},"source":["## VGG\n","\n","![](https://qph.fs.quoracdn.net/main-qimg-ba81c87204be1a5d11d64a464bca39eb)"]},{"cell_type":"code","metadata":{"id":"iYZ_aT6jAXrA","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zVcENC58ACak","colab_type":"text"},"source":["## ResNet (Deep Residual Network)"]},{"cell_type":"markdown","metadata":{"id":"6CLkYRrQAmXG","colab_type":"text"},"source":["Is learning better networks as easy as stacking more layers?\n","\n","The answer is no, in training deep networks we have the following problems:\n","\n","*   Vanishing / exploding  gradients,  which hamper convergence from the beginning (largely addressed by normalized initialization (e.g. Xavier) and intermediate normalization layers (e.g. Batch Norm))\n","*   Degradation problem:  with the network depth increasing, accuracy gets saturated (which might beun surprising)  and  then degrades  rapidly. Unexpectedly, such degradation is not caused by overfitting.\n","\n","The degradation problem is interesting. It indicates that not all systems are similarly easy to optimize. Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by construction to the deeper model: the added layers are identity mapping, and the other layers are copied from the learned shallower model.  The existence of this constructed solution indicates that a deeper model should produce no higher training error than its shallower counterpart. But experiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution (or unable to do so in feasible time).\n","\n","\\\\\n","\n","In a deep  residual  learning framework, instead  of  hoping  each  few  stacked  layers  directly  fit  a desired  underlying  mapping,  authors  explicitly  let  these  layers fit a residual mapping. More specifficaly, if the desired underlying mapping is H(x), instead of mapping it directly, authors let the stacked non linear layers fit another mapping of F(x) := H(x) âˆ’ x.\n","\n","They hypothesize that it is easier to optimize the residual mapping than to optimize the original, unreferenced mapping. \n","\n","To the extreme, if an identity mapping were optimal, it would be easier to push the residual to zero than to fit an identity mapping by a stack of nonlinear layers."]},{"cell_type":"markdown","metadata":{"id":"FHmzS_Z9ChYx","colab_type":"text"},"source":["![](https://drive.google.com/uc?export=view&id=1c0P4N4Ax9Ox2Wu6MSACXVKIzBZzz-SXq)"]},{"cell_type":"markdown","metadata":{"id":"Lc6QwUazBoby","colab_type":"text"},"source":["### What is residual layer\n","\n","![](https://drive.google.com/uc?export=view&id=1M82_hOttWl-_7_bHFkCxPivPJ3pm7USI)"]},{"cell_type":"markdown","metadata":{"id":"VcLppCl0RNAj","colab_type":"text"},"source":["### ResNet architecture"]},{"cell_type":"markdown","metadata":{"id":"rXMFyRZiBiru","colab_type":"text"},"source":["![](https://cdn-images-1.medium.com/max/1400/1*S3TlG0XpQZSIpoDIUCQ0RQ.jpeg)"]},{"cell_type":"markdown","metadata":{"id":"_qB-KrSWDL6B","colab_type":"text"},"source":["### Batch normalization\n","\n","[How to use BatchNorm with activation functions and dropout](https://stackoverflow.com/questions/39691902/ordering-of-batch-normalization-and-dropout)\n","\n","![](https://drive.google.com/uc?export=view&id=1iTOczI0iM-cr258vHyyN4t-gauslkOXL)"]},{"cell_type":"code","metadata":{"id":"SlQtVC-bACqS","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tult5Ym771hA","colab_type":"text"},"source":["# Tensorboard"]},{"cell_type":"markdown","metadata":{"id":"TP_IAfWy8dZe","colab_type":"text"},"source":["The computations you'll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, Google have included a suite of visualization tools called TensorBoard. You can use TensorBoard to visualize your TensorFlow graph, plot quantitative metrics about the execution of your graph, and show additional data like images that pass through it. When TensorBoard is fully configured, it looks like this:\n","\n","![](https://www.tensorflow.org/images/mnist_tensorboard.png)"]},{"cell_type":"markdown","metadata":{"id":"a0Z06e5V8qWq","colab_type":"text"},"source":["**In collab we run the tensorboard in the following way, where *logs/fit* is the path of directory where we saved logs of our model**"]},{"cell_type":"code","metadata":{"id":"I6SBEaeqotT1","colab_type":"code","colab":{}},"source":["%tensorboard --logdir logs/fit"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eo1vVjifQV93","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BRDEynh_QyX6","colab_type":"text"},"source":["# Create ResNet for CIFAR10 images classification"]},{"cell_type":"markdown","metadata":{"id":"yX0X3NDOQWHd","colab_type":"text"},"source":["## Load CIFAR10 dataset"]},{"cell_type":"code","metadata":{"id":"ex7NBT7KoxJc","colab_type":"code","colab":{}},"source":["cifar = keras.datasets.cifar10\n","\n","(X_train, y_train), (X_test, y_test) = cifar.load_data()\n","\n","X_train = X_train / 255\n","X_test = X_test / 255\n","\n","print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nxj8tmGnVF2A","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2edNA1uoQ8Tw","colab_type":"text"},"source":["## Define the network"]},{"cell_type":"markdown","metadata":{"id":"LA8fm7VtQ-jm","colab_type":"text"},"source":["### Define the function, that creates the residual layer\n","\n","The function should take 3 arguments:\n","*   **x_in** - input tensor\n","*   **channels_out** - number of channels of image returned by the layer\n","*   **strides** - layer stride size\n","\n","\\\\\n","\n","The function should:\n","1.  Take the *x_in* tensor and apply the  convolutional layer [keras.layers.Conv2d](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D), with *channels_out* filters, with kernel size equal to 3, same padding, stride size equal to *strides* and **without** any activation function.\n","2.  Add [Batch Normalization](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) to the output of the first layer.\n","3.  Use [ReLU](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU) activation function.\n","4.  Apply another convolutional layer with kernel size equal to 3, same padding, stride size equal to *strides* and **without** any activation function.\n","5.  Again use Batch Normalization.\n","6.  If shape of the tensor returned by function from point 5 is different than the shape of the input image, you should rescale the input image, by using the 1x1 convolutions (so the convolutional layer with kernel size equal to 1) with the propper stride.\n","7.  Add (rescaled) input image to the image returned from point 5.\n","6.  Apply ReLu activation.\n","\n","\\\\\n","\n","** Note: You can also try to implement residual layer, by defining the custom layer, instead of  creating this funcion **"]},{"cell_type":"code","metadata":{"id":"xr6MVg03VF4h","colab_type":"code","colab":{}},"source":["def residual_layer(x_in, channels_out, strides=1):\n","    ##\n","    \n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u1pPC8XnUFgy","colab_type":"text"},"source":["### Define the ResNet 20 architecture"]},{"cell_type":"markdown","metadata":{"id":"_VFWIiDpUI6g","colab_type":"text"},"source":["**Define the input layer with a proper input shape**"]},{"cell_type":"code","metadata":{"id":"hjlL71d8VF7g","colab_type":"code","colab":{}},"source":["x_in = ##"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HEWRCjJeUvCc","colab_type":"text"},"source":["**Apply the first convolutional layer**\n","\n","Apply classic (non residual convolutional layer) with 16 filters, kernel size equal to 3, same padding, stride size equal to 1. Remember about BatchNorm and ReLU activation."]},{"cell_type":"code","metadata":{"id":"Uc5FaeHOVF9q","colab_type":"code","colab":{}},"source":["x = ##"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P6Hb_lWBVk-q","colab_type":"text"},"source":["**Apply the first block of residual layers**\n","\n","Residual layers should work on image with 16 channels and 32x32 size.\n","\n","The block should be made of 3 residual layers."]},{"cell_type":"code","metadata":{"id":"PJvpnpl_k3ls","colab_type":"code","colab":{}},"source":["##"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2zOEmu29WX2b","colab_type":"text"},"source":["**Apply the second block of residual layers**\n","\n","Residual layers should work on image with 32 channels and 16x16 size.\n","\n","The block should be made of 3 residual layers."]},{"cell_type":"code","metadata":{"id":"u_NRjkRAj4RY","colab_type":"code","colab":{}},"source":["##"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lbN-KaBhWgoD","colab_type":"text"},"source":["**Apply the third block of residual layers**\n","\n","Residual layers should work on image with 64 channels and 8xx size.\n","\n","The block should be made of 3 residual layers."]},{"cell_type":"code","metadata":{"id":"HPgw4illkqql","colab_type":"code","colab":{}},"source":["##"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ARBB-xT8WlpX","colab_type":"text"},"source":["**Apply the global average pooling**\n","\n","[GlobalAveragePooling in tf.keras](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D)"]},{"cell_type":"code","metadata":{"id":"xodvd51GVGAZ","colab_type":"code","colab":{}},"source":["x = ##"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6AUSj1fBW5Pc","colab_type":"text"},"source":["**Apply the output layer**\n","\n","To the image returned by convolutional layers, we should apply the [dense layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense), with a proper number of units and activation function."]},{"cell_type":"code","metadata":{"id":"IfMWpWt_k1f0","colab_type":"code","colab":{}},"source":["x_out = ##"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"976VIButXYcY","colab_type":"text"},"source":["**Define keras model**\n","\n","You should pass the proper input and output tensors to the initializer."]},{"cell_type":"code","metadata":{"id":"DW_JpVmIlcmM","colab_type":"code","colab":{}},"source":["model = ##"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"az4Wj9J1Xhuy","colab_type":"text"},"source":["**Call the summary function**\n","\n","We could also plot the image of our model by calling the following functions:\n","\n","\n","```\n","keras.utils.plot_model(model, to_file='model.png', show_shapes=True)\n","plt.figure(figsize=(10,20))\n","img = Image.open('model.png')\n","plt.imshow(img)\n","```\n"]},{"cell_type":"code","metadata":{"id":"SozhZh52lcv6","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eywHWo3uYU9K","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qIYgYHIGYVIM","colab_type":"text"},"source":["### Train the model"]},{"cell_type":"markdown","metadata":{"id":"-DD_x_jJYXVA","colab_type":"text"},"source":["**Before training, you should conpile the model with a propper loss function and optimizer**"]},{"cell_type":"code","metadata":{"id":"5M5M9HWJmnG5","colab_type":"code","colab":{}},"source":["##"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2dfFXu50Yf94","colab_type":"text"},"source":["**The following line creates the tensorboard callback**\n","\n","With this callback, tensorboard can print the losses/metrics/graphs returned by our model.\n","\n","[TensorBoard callback](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard)"]},{"cell_type":"code","metadata":{"id":"ZUJfEZKdqWQB","colab_type":"code","colab":{}},"source":["log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x_iFyy5AY0v0","colab_type":"text"},"source":["**Train the model**\n","\n","In the original paper they used the batchsize equal to 128. Remember about TensorBoard callback."]},{"cell_type":"code","metadata":{"id":"eSaheHammnJp","colab_type":"code","colab":{}},"source":["##"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"X_tGOXkFotXN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fwUxiYmYZHz2","colab_type":"text"},"source":["**Note 1:** You could experiment with other resnet architectures, data augmentation (with using [tf datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to [train keras model](https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation#training_evaluation_from_tfdata_datasets)), regularization methods, dropout, optimizers, [learning rate schedulers](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler), in order to get the best classification result as you can."]},{"cell_type":"markdown","metadata":{"id":"ItT6IXaJaPJa","colab_type":"text"},"source":["**Note 2:** You could try to implement model training in more [tensorflow'ish way](https://www.tensorflow.org/alpha/guide/keras/training_and_evaluation#part_ii_writing_your_own_training_evaluation_loops_from_scratch)"]},{"cell_type":"code","metadata":{"id":"tgNDmHJootZj","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RmXDkl5_bEuH","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uUtk9R-IbFIH","colab_type":"text"},"source":["# Images sources\n","\n","Images used in this notebook comes from the following web pages and papers:\n","1.  https://www.researchgate.net/figure/AlexNet-CNN-architecture-layers_fig1_318168077\n","2.  https://www.quora.com/What-is-the-VGG-neural-network\n","3.  https://medium.com/@pierre_guillou/understand-how-works-resnet-without-talking-about-residual-64698f157e0c\n","4.  [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf) - Text is also inspired by this publication\n","5.  [BatchNorm paper](https://arxiv.org/pdf/1502.03167.pdf)\n"]},{"cell_type":"code","metadata":{"id":"-63a3Uf8otcM","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}