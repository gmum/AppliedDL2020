{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer_answers.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMK0/feSsdBvmGC8dB/pXZ7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"sGli-ZXYXhHW","colab_type":"code","colab":{}},"source":["try:\n","    # %tensorflow_version only exists in Colab.\n","    %tensorflow_version 2.x\n","except Exception:\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jAR9fGCQ6Dxg","colab_type":"code","colab":{}},"source":["import os\n","import sys\n","import math\n","import time\n","import itertools\n","\n","import tensorflow as tf\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from tensorflow import keras\n","from sklearn.preprocessing import OneHotEncoder\n","\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mtgJZ2raAr4F","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L8E0ogfXXpzR","colab_type":"text"},"source":["Code is based on: \\\\\n","[TensorFlow Transformer tutorial](https://www.tensorflow.org/tutorials/text/transformer) \\\\\n","[The annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)"]},{"cell_type":"code","metadata":{"id":"u4lPrbVEXoJ0","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HwdJyElyAdFc","colab_type":"text"},"source":["# Transformer"]},{"cell_type":"markdown","metadata":{"id":"50asXtfEBZMj","colab_type":"text"},"source":["![](https://www.tensorflow.org/images/tutorials/transformer/transformer.png)\n","\n","![](https://www.tensorflow.org/images/tutorials/transformer/multi_head_attention.png)\n","\n","![](https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png)\n","\n","\n","Encoder layer attention\n","![](http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_1.png)\n","\n","Decoder layer attention\n","![](http://nlp.seas.harvard.edu/images/the-annotated-transformer_119_11.png)"]},{"cell_type":"code","metadata":{"id":"X4Z-SQb1XoPf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uumulVZrAell","colab_type":"text"},"source":["# BERT - bidirectional pretraining of Transformer"]},{"cell_type":"markdown","metadata":{"id":"DgMVN7_XCoqp","colab_type":"text"},"source":["![](https://drive.google.com/uc?export=view&id=1LfLDPCHlovwwChNGPq8ArExk1uUshuVM)"]},{"cell_type":"code","metadata":{"id":"01uo2H9eXoNq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2aQ62OQXAtal","colab_type":"text"},"source":["# Transformer for other tasks"]},{"cell_type":"markdown","metadata":{"id":"d3VHhpyVC8jE","colab_type":"text"},"source":["Transformers and self-attention module are not used only in NLP. They are widely used in Deep Learning. \n","\n","\n","\n","*   Generative models - [Self-Attention Generative Adversarial Networks](https://arxiv.org/pdf/1805.08318.pdf), [Music Transformer](https://openreview.net/pdf?id=rJe4ShAcF7)\n","*   Computer Vision - [Image Transformer](https://arxiv.org/pdf/1802.05751.pdf), [Stand-Alone Self-Attention in Vision Models](https://arxiv.org/pdf/1906.05909.pdf), [Attention Augmented Convolutional Networks](http://openaccess.thecvf.com/content_ICCV_2019/papers/Bello_Attention_Augmented_Convolutional_Networks_ICCV_2019_paper.pdf)\n","*   Graphs processing - [Graph Attention Networks](https://arxiv.org/pdf/1710.10903.pdf), [Molecule Attention Transformer](https://arxiv.org/pdf/2002.08264.pdf)\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"W2qdF6yMcEnY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8sLHQCpcJNT","colab_type":"text"},"source":["## IMDB Movie reviews dataset for sentiment classification - once again"]},{"cell_type":"markdown","metadata":{"id":"rs_TU0EjcNLf","colab_type":"text"},"source":["**Overview**\n","\n","This dataset contains movie reviews along with their associated binary sentiment polarity labels. It is intended to serve as a benchmark for sentiment classification.\n","\n","**Dataset**\n","\n","The core dataset contains 50,000 reviews split evenly into 25k train and 25k test sets. The overall distribution of labels is balanced (25k pos and 25k neg).\n","\n","In the entire collection, no more than 30 reviews are allowed for any given movie because reviews for the same movie tend to have correlated ratings. Further, the train and test sets contain a disjoint set of movies, so no significant performance is obtained by memorizing movie-unique terms and their associated with observed labels. In the labeled train/test sets, a negative review has a score <= 4 out of 10, and a positive review has a score >= 7 out of 10. Thus reviews with more neutral ratings are not included in the train/test sets.\n","\n","**Model**\n","\n","This time to classify reviews, we will use the Transformer model for classification (just encoder part)."]},{"cell_type":"code","metadata":{"id":"DB2liS0-6D2m","colab_type":"code","colab":{}},"source":["from keras.datasets import imdb\n","\n","(x_train, y_train), (x_test, y_test) = imdb.load_data()\n","\n","x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=200, padding='post')\n","\n","MAX_SEQUENCE_LEN = x_train.shape[1]\n","WORDS_IN_CORPORA = np.max(x_train) + 1\n","\n","x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_SEQUENCE_LEN, padding='post', truncating='post')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L1-SsGpv6D4q","colab_type":"code","colab":{}},"source":["x_train.shape, y_train.shape, x_test.shape, y_test.shape, WORDS_IN_CORPORA"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJ0Rolk56D7y","colab_type":"code","colab":{}},"source":["words2index = imdb.get_word_index()\n","index2word = {v: k for k,v in words2index.items()}\n","imdb_words = [v for k,v in sorted(index2word.items())]\n","len(imdb_words)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vExVqysZ6D-p","colab_type":"code","colab":{}},"source":["imdb_words[0:5]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"b1tt8bZJKVu8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bd8btX-WKV2C","colab_type":"text"},"source":["## Positional encoding"]},{"cell_type":"markdown","metadata":{"id":"xlU9qe5BdT1s","colab_type":"text"},"source":["Since this model doesn't contain any recurrence or convolution, positional encoding is added to give the model some information about the relative position of the words in the sentence.\n","\n","The positional encoding vector is added to the embedding vector. Embeddings represent a token in a d-dimensional space where tokens with similar meaning will be closer to each other. But the embeddings do not encode the relative position of words in a sentence. So after adding the positional encoding, words will be closer to each other based on the similarity of their meaning and their position in the sentence, in the d-dimensional space.\n","\n","$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}})$ \\\\\n","$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})$\n","\n","![](http://nlp.seas.harvard.edu/images/the-annotated-transformer_49_0.png)"]},{"cell_type":"code","metadata":{"id":"k2tGj5tP6EEw","colab_type":"code","colab":{}},"source":["def get_angles(pos, i, d_model):\n","  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n","  return pos * angle_rates"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gu-waXPe6EHI","colab_type":"code","colab":{}},"source":["def positional_encoding(text_len, d_model):\n","  angle_rads = get_angles(np.arange(text_len)[:, np.newaxis],\n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model) # (text_len, d_model)\n","  \n","  # apply sin to even indices in the array; 2i\n","  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","  \n","  # apply cos to odd indices in the array; 2i+1\n","  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","    \n","  pos_encoding = angle_rads[np.newaxis, ...] # (1, text_len, d_model)\n","    \n","  return tf.cast(pos_encoding, dtype=tf.float32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sqyZuy6MKaYu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dDRlpQh4Lkub","colab_type":"text"},"source":["## Masking"]},{"cell_type":"markdown","metadata":{"id":"sepOlIxJgH_T","colab_type":"text"},"source":["Mask all the pad tokens in the batch of sequence. It ensures that the model does not treat padding as the input during the calculation of self-attention. \n","\n","The mask indicates where pad value 0 is present: it outputs a 1 at those locations, and a 0 otherwise."]},{"cell_type":"code","metadata":{"id":"aoXYwDUMLjt8","colab_type":"code","colab":{}},"source":["def create_padding_mask(seq):\n","  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n","  \n","  # add extra dimensions to add the padding\n","  # to the attention logits.\n","  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6XeIfMkLjwl","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fXdrh6_rMkvw","colab_type":"text"},"source":["## Scaled dot product self-attention"]},{"cell_type":"markdown","metadata":{"id":"-ECtor2hhfCA","colab_type":"text"},"source":["![](https://www.tensorflow.org/images/tutorials/transformer/scaled_attention.png)\n","\n","The attention function used by the transformer takes three inputs: Q (query), K (key), V (value). The equation used to calculate the attention weights is:\n","\n","$\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n","\n","An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n","\n","The dot-product attention is scaled by a factor of square root of the depth. This is done because for large values of depth, the dot product grows large in magnitude pushing the softmax function where it has small gradients resulting in a very hard softmax. "]},{"cell_type":"code","metadata":{"id":"4KA8U5mKKaa0","colab_type":"code","colab":{}},"source":["def scaled_dot_product_attention(q, k, v, mask):\n","  \"\"\"Calculate the attention weights.\n","  q, k, v must have matching leading dimensions.\n","  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n","  The mask has different shapes depending on its type(padding or look ahead) \n","  but it must be broadcastable for addition.\n","  \n","  Args:\n","    q: query shape == (..., seq_len_q, depth)\n","    k: key shape == (..., seq_len_k, depth)\n","    v: value shape == (..., seq_len_v, depth_v)\n","    mask: Float tensor with shape broadcastable \n","          to (..., seq_len_q, seq_len_k). Defaults to None.\n","    \n","  Returns:\n","    output, attention_weights\n","  \"\"\"\n","\n","  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n","  \n","  # scale matmul_qk\n","  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n","\n","  # add the mask to the scaled tensor.\n","  if mask is not None:\n","    scaled_attention_logits += (mask * -1e9)  \n","\n","  # softmax is normalized on the last axis (seq_len_k) so that the scores\n","  # add up to 1.\n","  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n","\n","  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n","\n","  return output, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UjQOP1zjLjzN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lnisZvQhQw4Z","colab_type":"text"},"source":["## Multi-head attention"]},{"cell_type":"markdown","metadata":{"id":"hNxzivw1i7Jn","colab_type":"text"},"source":["Multi-head attention consists of four parts:\n","\n","\n","\n","1.   Linear layers and split into heads.\n","2.   Scaled dot-product attention.\n","3.   Concatenation of heads.\n","4.   Final linear layer.\n","\n","Each multi-head attention block gets three inputs; Q (query), K (key), V (value). These are put through dense layers and split up into multiple heads.\n","\n","The sefl-attention mechanism defined above is applied to each head. An appropriate mask must be used in the attention step. The attention output for each head is then concatenated and put through a final dense layer.\n","\n","Instead of one single attention head, Q, K, and V are split into multiple heads because it allows the model to jointly attend to information at different positions from different representational spaces. After the split each head has a reduced dimensionality, so the total computation cost is the same as a single head attention with full dimensionality."]},{"cell_type":"markdown","metadata":{"id":"rqNNjWM8iV0m","colab_type":"text"},"source":["![](http://nlp.seas.harvard.edu/images/the-annotated-transformer_38_0.png)"]},{"cell_type":"code","metadata":{"id":"c1Vo9A6wQs1v","colab_type":"code","colab":{}},"source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads):\n","    super(MultiHeadAttention, self).__init__()\n","    self.num_heads = num_heads\n","    self.d_model = d_model\n","    \n","    assert d_model % self.num_heads == 0\n","    \n","    self.depth = d_model // self.num_heads\n","    \n","    self.wq = tf.keras.layers.Dense(d_model)\n","    self.wk = tf.keras.layers.Dense(d_model)\n","    self.wv = tf.keras.layers.Dense(d_model)\n","    \n","    self.dense = tf.keras.layers.Dense(d_model)\n","        \n","  def split_heads(self, x, batch_size):\n","    \"\"\"Split the last dimension into (num_heads, depth).\n","    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n","    \"\"\"\n","    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","    return tf.transpose(x, perm=[0, 2, 1, 3])\n","    \n","  def call(self, v, k, q, mask):\n","    batch_size = tf.shape(q)[0]\n","    \n","    q = self.wq(q)  # (batch_size, seq_len_q, d_model)\n","    k = self.wk(k)  # (batch_size, seq_len_k, d_model)\n","    v = self.wv(v)  # (batch_size, seq_len_v, d_model)\n","    \n","    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n","    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n","    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n","    \n","    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n","    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n","    scaled_attention, attention_weights = scaled_dot_product_attention(\n","        q, k, v, mask)\n","    \n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n","\n","    concat_attention = tf.reshape(scaled_attention, \n","                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n","\n","    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n","        \n","    return output, attention_weights"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"aW7tSuDQQs5L","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"njLOUTDcQ-6F","colab_type":"text"},"source":["## Position-wise feed forward network"]},{"cell_type":"markdown","metadata":{"id":"Ih61xDDvjvc9","colab_type":"text"},"source":["Point wise feed forward network consists of two fully-connected layers with a ReLU activation in between.\n","\n","While the linear transformations are the same across different positions, they use different parameters from layer to layer."]},{"cell_type":"code","metadata":{"id":"Hj39GUFgQs8S","colab_type":"code","colab":{}},"source":["def position_wise_feed_forward_network(d_model, dff):\n","  return tf.keras.Sequential([\n","      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n","      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n","  ])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zlxg_1-IRm-m","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mk6pyCRLRnFF","colab_type":"text"},"source":["## Encoder Layer"]},{"cell_type":"markdown","metadata":{"id":"kebpBBGfkHNC","colab_type":"text"},"source":["Each encoder layer consists of sublayers:\n","\n","1.    Multi-head attention (with padding mask)\n","2.    Point wise feed forward networks.\n","\n","Each of these sublayers has a residual connection around it followed by a layer normalization. Residual connections help in avoiding the vanishing gradient problem in deep networks.\n","\n","The output of each sublayer is normalised by the Layer Normalization method and equals \n","\n","```\n","LayerNorm(x + Sublayer(x))\n","```\n","\n","There are N encoder layers in the transformer."]},{"cell_type":"code","metadata":{"id":"pJpkUphrQs-m","colab_type":"code","colab":{}},"source":["class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, d_model, num_heads, dff, rate=0.1):\n","    super(EncoderLayer, self).__init__()\n","\n","    self.mha = MultiHeadAttention(d_model, num_heads)\n","    self.ffn = position_wise_feed_forward_network(d_model, dff)\n","\n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    \n","    self.dropout1 = tf.keras.layers.Dropout(rate)\n","    self.dropout2 = tf.keras.layers.Dropout(rate)\n","    \n","  def call(self, x, mask):\n","\n","    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n","    attn_output = self.dropout1(attn_output)\n","    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n","    \n","    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n","    ffn_output = self.dropout2(ffn_output)\n","    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n","    \n","    return out2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJR2fNMnQtAY","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vmxMGtYRRtti","colab_type":"text"},"source":["## Encoder"]},{"cell_type":"markdown","metadata":{"id":"cB44D5EMkx_k","colab_type":"text"},"source":["The Encoder consists of:\n","\n","1.    Input Embedding\n","2.    Positional Encoding\n","3.    N encoder layers\n","\n","The input is put through an embedding which is summed with the positional encoding. The output of this summation is the input to the encoder layers. The output of the encoder is the input to the classification head."]},{"cell_type":"code","metadata":{"id":"W8xXfJGdRu-k","colab_type":"code","colab":{}},"source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size,\n","               maximum_position_encoding, rate=0.1):\n","    super(Encoder, self).__init__()\n","\n","    self.d_model = d_model\n","    self.num_layers = num_layers\n","    \n","    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n","    self.pos_encoding = positional_encoding(maximum_position_encoding, \n","                                            self.d_model)\n","    \n","    \n","    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n","                       for _ in range(num_layers)]\n","  \n","    self.dropout = tf.keras.layers.Dropout(rate)\n","        \n","  def call(self, x, mask):\n","\n","    seq_len = tf.shape(x)[1]\n","    \n","    # adding embedding and position encoding.\n","    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","\n","    x = self.dropout(x)\n","    \n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x, mask)\n","    \n","    return x  # (batch_size, input_seq_len, d_model)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RPkcadbiRve8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AB7pEliaSZNg","colab_type":"text"},"source":["## Create the classification Transformer"]},{"cell_type":"markdown","metadata":{"id":"lqP0v_P7n9EO","colab_type":"text"},"source":["Classification Transformer consists of the encoder and a classification denselayer. The first position embedding of the encoder's output is the input to the linear layer and its output is returned."]},{"cell_type":"code","metadata":{"id":"BuRBBZwfRvhf","colab_type":"code","colab":{}},"source":["class Transformer(tf.keras.Model):\n","  def __init__(self, num_layers, d_model, num_heads, dff, vocab_size, max_len, rate=0.1):\n","    super(Transformer, self).__init__()\n","\n","    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n","                           vocab_size, max_len, rate)\n","\n","    self.final_layer = tf.keras.layers.Dense(1)\n","    \n","  def call(self, inp):\n","    enc_padding_mask = create_padding_mask(inp)\n","\n","    enc_output = self.encoder(inp, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n","    \n","    final_output = self.final_layer(enc_output[:,0,:])  # (batch_size, target_vocab_size)\n","    \n","    return final_output"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HWuNTZrTZFaZ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pKIx4h-KZFiZ","colab_type":"text"},"source":["## Optimizer"]},{"cell_type":"markdown","metadata":{"id":"2o9vHZB8oU4b","colab_type":"text"},"source":["Optimizer is Adam with a custom learning rate scheduler according to the formula in the paper.\n","\n","$lrate = d_{\\text{model}}^{-0.5} \\cdot\n","  \\min({step\\_num}^{-0.5},\n","    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})$\n","\n","\n","Below you can see the example of the curves of this model for different model sizes and for optimization hyperparameters.\n","\n","![](http://nlp.seas.harvard.edu/images/the-annotated-transformer_69_0.png)\n"]},{"cell_type":"code","metadata":{"id":"ZlM6yhZARvj0","colab_type":"code","colab":{}},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","  def __init__(self, d_model, warmup_steps=4000):\n","    super(CustomSchedule, self).__init__()\n","    \n","    self.d_model = d_model\n","    self.d_model = tf.cast(self.d_model, tf.float32)\n","\n","    self.warmup_steps = warmup_steps\n","    \n","  def __call__(self, step):\n","    arg1 = tf.math.rsqrt(step)\n","    arg2 = step * (self.warmup_steps ** -1.5)\n","    \n","    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s51ERyfTZJDm","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tnw8b_tZXGdC","colab_type":"text"},"source":["## Define and compile the model"]},{"cell_type":"code","metadata":{"id":"mmLX_c0Wg9Oq","colab_type":"code","colab":{}},"source":["num_layers = 4\n","d_model = 128\n","num_heads = 4\n","dff = 512\n","vocab_size = WORDS_IN_CORPORA\n","max_len = MAX_SEQUENCE_LEN\n","warmup_steps = 2000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Sh7w0ZpVbaY","colab_type":"code","colab":{}},"source":["transformer = Transformer(num_layers, d_model, num_heads, dff, vocab_size, max_len)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PF-5AgSTZHBq","colab_type":"code","colab":{}},"source":["learning_rate = CustomSchedule(d_model, warmup_steps)\n","\n","optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n","                                     epsilon=1e-9)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7s3L44WsUuZd","colab_type":"code","colab":{}},"source":["sequence_input = keras.layers.Input(shape=(MAX_SEQUENCE_LEN,), dtype='int32')\n","sequence_output = transformer(sequence_input)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"py5qIJSpUuem","colab_type":"code","colab":{}},"source":["model = keras.models.Model(inputs=[sequence_input], outputs=[sequence_output])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hiUVoCtUuhu","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AbQ5dI1AV3yI","colab_type":"code","colab":{}},"source":["model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n","              optimizer=optimizer, \n","              metrics=[\"accuracy\"])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7tVn8q7xXDf-","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X2MFIB9tUuTn","colab_type":"text"},"source":["## Train the model"]},{"cell_type":"code","metadata":{"id":"9keI451rV300","colab_type":"code","colab":{}},"source":["model.fit(x_train, y_train, epochs=10, batch_size=32,\n","          validation_data=(x_test, y_test))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8OxDa4CNV33h","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cT-S3gIFJ9Fz","colab_type":"text"},"source":["# Exercise"]},{"cell_type":"markdown","metadata":{"id":"l9b6-tGlKCuv","colab_type":"text"},"source":["Find some nice attention patterns in the data and visualise it."]},{"cell_type":"code","metadata":{"id":"7cAt23JRV36F","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}